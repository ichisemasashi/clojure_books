[UP](README.md)

## オプション資料。他のタイプのDeepLearning4Jの組み込みレイヤーのドキュメント

[DL4Jに内蔵されているレイヤー・クラスのドキュメント](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/package-tree.html)は、今のところ必要以上のものであると思われますので、私が時々使用する他のタイプのレイヤーについて確認してみましょう。前節で使用した簡単な例では、2種類のレイヤーを使用しました。


[org.deeplearning4j.nn.conf.layers.DenseLayer](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/DenseLayer.html) - 前後の層のすべてのニューロンとの接続を維持する、つまり "完全接続 "されている。

[org.deeplearning4j.nn.conf.layers.OutputLayer](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/OutputLayer.html) - 前の層を経由してバックプロパゲーションの計算を開始するためのビルトインの動作を持ちます。

より多くの深層学習対応アプリケーションを構築する際には、どのような要件に応じて、以下のDl4Jレイヤークラスの少なくとも一部を使用する必要があるでしょう。


[org.deeplearning4j.nn.conf.layers.AutoEncoder](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/AutoEncoder.html) - データからノイズを取り除くためによく使用されます。オートエンコーダーは、オートエンコーディング層のニューロンの数を減らしながら、目標とするトレーニング出力値を入力トレーニング値と等しくすることで機能します。この層は、データの簡潔な表現を学習したり、どの特徴が重要であるかを学習してデータを「一般化」する。

[org.deeplearning4j.nn.conf.layers.CapsuleLayer](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/CapsuleLayer.html) - カプセルネットワークは、畳み込みモデルのより効率的なバージョンであることを試みています。畳み込みネットワークは検出された特徴の位置情報を破棄しますが、カプセルモデルはこの情報を維持して使用します。

[org.deeplearning4j.nn.conf.layers.Convolution1D](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/Convolution1DLayer.html) - 1次元の畳み込み層は、1次元の特徴検出器を学習します。学習された層は特徴を認識することを学習しますが、特徴がどこにあるかという情報は破棄されます。これらは、信号データや自然言語処理における単語トークンなどのデータ入力ストリームによく使われます。

[org.deeplearning4j.nn.conf.layers.Convolution2D](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/Convolution2D.html) - 2次元の畳み込み層は、2次元の特徴検出器を学習します。学習された層は特徴を認識することを学習しますが、特徴がどこにあるかという情報は破棄されます。写真の中に、ある種類の物体が写っているかどうかを認識するのに使われます。なお、鼻や口などの特徴は認識されますが、入力画像のどこにあるかは関係ありません。例えば、人の顔の画像を切り取って、耳を画像の中央に、口を左上に移動させても、ソフトマックス出力層を使えば、すべての出力クラスの値の合計が1になるので、確率として解釈できるクラスラベルが生成されるので、ある程度の確率で顔が写っていると予測されます。

[org.deeplearning4j.nn.conf.layers.EmbeddingLayer](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/EmbeddingLayer.html) - エンベッディング層は、入力データを整数データに変換するために使用されます。私が最も頻繁に使用するエンベッディング層は、トレーニングデータの各単語に整数値を割り当てるワードエンベッディングです。このデータは「1ホットエンコード」することができ、単語を処理する場合、分類器のトレーニングデータに5000個のユニークな単語があるとすると、エンベッディング層には5001個のニューロンがあり、各単語に1個、トレーニングデータにないすべての単語を表すために1個のニューロンがあります。単語のインデックス（インデックスはゼロベース）が例えば117であれば、インデックス117のニューロンの活性化値は1に設定され、層内の他のすべてのニューロンはゼロに設定されます。

[org.deeplearning4j.nn.conf.layers.FeedForwardLayer](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/FeedForwardLayer.html) - これは、ほとんどの特殊なタイプのフィードフォワード層のスーパークラスなので、クラスのリファレンスを読むことをお勧めします。

[org.deeplearning4j.nn.conf.layers.DropoutLayer](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/DropoutLayer.html) - ドロップアウト層は、新しい入力パターンを学習することで、ネットワークが以前に学習したパターンを忘れてしまうのを防ぐのに非常に役立ちます。各トレーニングバッチごとに、ドロップアウト層のニューロンの一部がオフになり、トレーニングバッチサイクルの間、重みが更新されません。ドロップアウトの開発は、深層学習ネットワークが多くの層と大量の学習データに対応できるようにするための歴史的な鍵となりました。

[org.deeplearning4j.nn.conf.layers.LSTM](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/LSTM.html) - LSTMレイヤーは、レイヤーが記憶できるものの時間的記憶を拡張するために使用されます。LSTMは、データストリームを通過させるために入力ウィンドウを使用するRNNモデルを改良したもので、RNNモデルは、この時間的サンプリングウィンドウの内側にあるものだけを使用することができます。

[org.deeplearning4j.nn.conf.layers.Pooling1D](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/Pooling1D.html) - 1次元プーリング層は、ダウンサンプリングによって長い入力を短い出力に変換します、つまり、入力接続よりも出力接続が少なくなります。

[org.deeplearning4j.nn.conf.layers.Pooling2D](https://deeplearning4j.org/api/latest/org/deeplearning4j/nn/conf/layers/Pooling2D.html) - 2次元プーリング層は、ダウンサンプリングによって、より大きな2次元のデータ入力配列をより小さな出力の2次元配列に変換する。



[UP](README.md)

[NEXT](01-03.md)