# Chapter 1: はじめに

この論文では、Schemeプログラミング言語システムの3つの実装モデルを紹介します。この3つのモデルは、ヒープベース、スタックベース、文字列ベースと呼ばれている。これは、1つ目のモデルが重要なデータ構造のヒープ配置に、2つ目のモデルがスタック配置に、3つ目のモデルが文字列配置に主に依存しているためである。ヒープベースのモデルはよく知られており、1975年にSchemeが登場して以来、ほとんどのScheme処理で採用されています[Sus75]。スタックベースのモデルと文字列ベースのモデルは新しいもので、ここで初めて完全に説明します。ヒープベースのモデルでは、コールフレームと変数バインディングを格納するためにヒープを使用する必要がありますが、スタックベースとストリングベースのモデルでは、同じ情報を格納するためにスタックやストリングを使用することができます。スタックベースのモデルでは、ヒープベースのモデルで必要とされるヒープの割り当てのほとんどを回避することができ、ほとんどのSchemeプログラムの実行に必要なスペースと時間を削減することができます。文字列ベースのモデルは、スタックとヒープの両方の割り当てを回避し、プログラムの特定の部分の同時評価を容易にします。スタックベースのモデルは、伝統的なシングルプロセッサコンピュータでの使用を想定しており、文字列ベースのモデルは、文字列削減によってプログラムを実行する小粒のマルチプロセッサコンピュータでの使用を想定しています。

スタックベースのモデルを最初に採用したのは、1983年と1984年に設計・実装した筆者のChez Schemeシステムでした。その後、PC Scheme [Bar86]やOrbit [Kra86]など、実装された他のシステムでも同じような手法が採用されています。また、Chez Schemeとほぼ同時期に独自に開発されたMLの実装[Car83, Car84]でも、同様の手法が採用されています。文字列ベースのモデルはまだ実装されていませんが、シーケンシャル・コンピュータ上での解釈によるテストは行われています。このモデルは、Mag ́o [Mag79, Mag79a, Mag84]のFFPマシン用のSchemeの実装が実現した暁には、採用される予定だ。FFPマシンは、BackusのFFP言語[Bac78]で書かれたプログラムを直接実行する、小粒のマルチプロセッサです。

Schemeは、λ計算機[Chu41, Cur58]をベースにしたLispプログラミング言語[McC60]の一種です。1975年にSteeleとSussmanによって発表され、その後大きな変化を遂げてきました[Sus75, Ste78, Ree86, Dyb87]。Schemeは、多くのLisp方言とは異なり、字句にスコープを持ち、ブロック構造を持ち、関数をファーストクラスのデータオブジェクトとしてサポートし、継続をファーストクラスのデータオブジェクトとしてサポートしています(*1)。人気の高いLispの方言であるCommon Lisp[Ste84]は、Schemeに多少の影響を受けており、字句のスコープと第一級関数をサポートしていますが、継続をサポートしていません。MLプログラミング言語[Car83a, Mil84, Gor79]は、多くの点でSchemeと似ており、字句のスコープと第一級関数をサポートしていますが、継続と変数の代入はありません。このような類似点があるため、本論文で紹介するアイデアの多くは、 SchemeだけでなくCommon LispやMLにも当てはまります。

この論文では、各実装モデルのいくつかのバリエーションを紹介します。これらの変種は、表現を単純化するとともに、Schemeと似ているが同じではない他の言語にも有用な代替モデルを提供することを目的としています。それぞれのモデルや変種は、主要なデータ構造の表現、ソースレベルのプログラムからオブジェクトレベルのプログラムへの変換、オブジェクトレベルのプログラムの評価を扱っています。翻訳処理と評価処理のほとんどは、その一部が実際のSchemeコードで記述されており、実行可能な仕様となっています。このコードに基づいてSchemeの完全な実装を行うことも可能ですが、表現を単純化して焦点を絞るために、ほとんどの詳細は省略されています。

この論文では、実用的ですぐに役立つ貢献をしている。それは、Schemeのスタックベースのモデルを説明したことです。これにより、シーケンシャルコンピュータ上のScheme処理系は、Algol 60 [Nau63]、Pascal [Jen74]、C [Ker78]などのブロック構造言語の処理系と同じように、標準的なスタックを使用することができます。ほとんどのSchemeシステムで採用されているヒープベースのモデルでは、コールフレームやバインディング環境をヒープに割り当てる必要があり、その結果、システムの速度とメモリ使用量が増大します。クロージャやコンティニュエーションをサポートするためには、スタックフレームのヒープ割り当てが必要だと考えられていました。

もう一つの貢献は、SchemeがMag ́oのFFPマシンや一般的な文字列還元マシン上で効率的に動作することを可能にする、Schemeの文字列ベースのモデルを記述したことです。このモデルは、FFPマシンが実現したときに役立つだけでなく、他の小粒径のマルチプルプロセッサでも有用であることが期待されます。第二の貢献は、Schemeをサポートする新しいFFPの説明と、FFPとFFPマシンを使った他の言語のサポートへの対応です。

第三の貢献は、最も単純なヒープベースのシステムから、最も複雑なスタックベースや文字列ベースのシステムまで、一連の代替実装モデルを詳細に記述し、比較したことです。ある状況下では、それぞれのモデルが他のモデルよりも優れているかもしれません。また、Schemeに最適なものもあれば、Schemeとは異なる言語に適したものもあります。

スタックベースと文字列ベースのモデルが効率的なScheme処理を可能にしているのは、Schemeが命令型ではなく関数型のプログラミング技術を推奨しているからでもある。つまり、典型的なSchemeプログラムは、ステートメントやループではなく、主に関数や再帰に依存しており、変数の代入も少ない傾向にある。代入は認められているが、Schemeのコードにはあまり登場しない。スタックベースモデルと文字列ベースモデルは、この点を利用して、代入のないコードの速度を向上させる一方で、代入を多用するコードにはペナルティを与える可能性があります。

本章の残りの部分では、関数型プログラミング言語、関数型言語の実装技術、関連するマルチプルプロセッサシステムの背景について説明します。第2章では、Schemeの詳細、構文、研究に値する特徴を説明しています。第3章、第4章、第5章では、ヒープベース、スタックベース、文字列ベースの技術を紹介しています。第6章では、結論とさらなる研究のためのアイデアを紹介しています。最後に、付録Aでは、スタックベースのモデルとヒープベースのモデルを比較し、本論文を完成させています。この章では、4つの単純なプログラムを対象に、2つのモデルを経験的に比較した結果を示し、2つのモデルのコンパイラが生成する可能性のある命令シーケンスを比較しています。

## 1.1 関数型プログラミング言語

多くのコンピュータプログラムは、命令型と関数型の2つのカテゴリーに分けられる。命令型プログラムは、ステートメントごとに状態を変化させ、ステートメント指向のループやサブルーチン（副次的な処理を行い、必ずしも値を返さない手続き）を用いてプログラムを制御する。関数型プログラムは、状態を変化させることなく、式指向の方法で値を計算し、関数（単に値を計算して返す手続き）、再帰、マッピングなどをプログラム制御に用いる。プログラミング言語は、その言語で記述できるプログラムのスタイルによって、通常、この2つのカテゴリーに分類され、言語が提供する機能によって決定される。命令型プログラミング言語では、宣言（手続きや変数）、代入、ループ制御、条件文、サブルーチンや関数の呼び出し、算術式などの文が主な機能となる。一方、関数型プログラミング言語では、結合式、条件式、関数呼び出し、算術式などの式が主な特徴である。
関数型プログラミング言語は、命令型プログラミング言語に比べていくつかの利点がある。

1. Functional programs are simpler. Functional programs are built from expressions in a natural, recursive fashion. Imperative programs are built from complex statements, or commands, combined with expressions. Certain contexts require expressions while others require statements. Statements are almost never allowed within expressions.
2. Functional programs are generally easier to understand. Each piece of the program can be taken apart from its context and studied separately from the remainder of the program, because there is little or no state affecting that piece of the program.
3. Correctness proofs can be applied more easily to functional programs because of the regularity of structure and lack of state.
4. Local variables are simpler and never uninitialized in functional programs. A variable is a name for a value rather than a storage location. This value is established when the variable is bound (declared). In an imperative program, the initial assignment is typically separated from its binding (declaration).
5. Alternative evaluation orders are possible in functional programming languages. The order of execution of two independent expressions of a program is not important, and the absence of state ensures that many expressions are independent (for example, the arguments to a function application). Such expressions may even be executed in parallel. Furthermore, an expression whose value is never required need never be executed at all.

FFP, the related FP [Bac78], KRC [Tur79, Tur82], and Miranda [Tur86] are examples of functional languages.
In spite of their attractive semantic properties and potential ease of implementation on parallel computers, there are some programs that are not easily expressed in functional languages, i.e., programs that require the concept of state. However, many programs are expressible and stylistically more attractive when written in a purely functional style. Some languages encourage a functional programming style but allow the use of imperative variable assignments when necessary. Such languages support the features that make programming in a functional style possible and omit features that discourage functional programming (such as loops, gotos, and statement-oriented conditionals). Scheme is one such language, as are Lisp, ML, APL [Ive62], and ISWIM [Brg81].
Chapter 2 discusses and illustrates the use of a functional subset of Scheme, but gives some examples of problems that do not easily lend themselves to a functional style. The Scheme programs used to describe translation and evaluation in Chapters 3, 4, and 5 are written where possible in a functional style, using assignments only sparingly. They help to demonstrate that Scheme encourages programs to be written in a functional style while still allowing assignments when necessary.

## 1.2 Functional Programming Language Implementations

Because Scheme is closer in spirit to functional languages than to imperative languages, it is useful to consider methods commonly used to implement functional languages. Functional languages may be implemented in several different ways on a sequential computer. The most common way is the construction of an interpreter. An interpreter requires modeling of the variable environment (if any), handling of any special syntax, providing for function application, and providing any run time support (such as storage management) [Wis82]. A related alternative is to compile the source program into a lower-level language (perhaps machine code) and to interpret programs in the lower-level language.
For languages without variables, string or graph reduction is possible, either in a sequential or parallel processor. With string reduction [Ber75], the program is represented as a string of symbols. Evaluation proceeds by replacing each reducible expression with its value, working within the string itself. Graph reduction [Wad71, Rev84, Tur79] is similar, except that the program is represented by a graph with common subexpressions sharing the same node (that is, identical subexpressions are not duplicated as they would be by the flat string representa-
tion). The main difference here is that such expressions are reduced only once.
For functional languages with variables, one alternative is the use of a combinator approach, such as described by Turner [Tur79]. Combinators are (typically) simple functions with no free variables. Two combinators, called S and K, are sufficient to describe any function in the λ-calculus [Cur58]. Once a program has been converted into a composition of S and K combinators, it may be reduced by a string or graph reduction machine. Hughes describes the use of more complex “super-combinators,” to gain a more compact and efficient translation of the input program [Hug82].
When evaluating programs in a functional language, one has the opportunity to use lazy evaluation [Fri76, Hen76, Tur79]. Lazy evaluation, sometimes referred to as demand-driven or call-by-need evaluation, promises to evaluate only those subexpressions necessary to complete the problem. This approach is semantically valid because an expression whose value is not required, and that does not cause any side-effects (a requirement of functional languages) cannot affect the computation. Such an expression may as well be left unevaluated. Both the interpretation and combinator approaches lend themselves to lazy evaluation strategies.
Languages such as Scheme have somewhat different requirements. In particular, it is not generally possible to use lazy evaluation. Not only must every expression that may cause a side-effect be evaluated, but also the ordering of the evaluation must be preserved. With lazy evaluation, the order of evaluation is unpredictable, depending upon what is needed when. Because of the requirement for a binding environment where changes to the values of variables may be recorded, reduction mechanisms are not easily adapted to languages that allow assignments. Also, combinators cannot easily be used to remove variables if the variables can be assigned.
This leaves the direct interpretation and compilation (to traditional machine code) approaches. Many Scheme systems have been developed [Abe84, Bar86, Cli84, Dyb83, Fri84, Kra86, Sus75, Ste77, Ste78], most of them using some combination of interpretation and compilation. There are also many descriptions of other Lisp implementations in the literature that use one or both of these approaches; these implementations are not relevant here since they do not address support for first-class closures, first-class continuations, or certain other Scheme features. Because of the need to support first-class functions and continuations, most implementations allocate call frames and binding environments in a heap.
An optimization of the typical environment structure used in Scheme programs was given by McDermott [McD80]. McDermott suggested that heap allocation of environments happen only when necessary, and was able to retain some variables on the stack. McDermott did not handle full continuations, but suggested that even in the presence of full continuations, a similar avoidance of heap allocation might be possible.
The T language developed at Yale was based on Scheme, but its designers avoided heap allocation of call frames by omitting full continuations from the early versions of the language. To quote from the 1982 Lisp Conference paper:

> “As a concession to efficient implementation on standard architectures, escape procedures are not valid outside the dynamic extent of the CATCH-expression which creates them; this ensures that the control stack behaves in a stacklike way, unlike in Scheme, where the control stack must be heap-allocated” [Ree82].

This dissertation shows that this need not be the case, as does a recent paper describing the latest implementation of T [Kra86].
Similar heap-allocated stack frames have been used in Smalltalk [Gol83, Ing78] implementations because stack frame objects may be retained indefinitely in a manner similar to general continuations.
Cardelli independently introduced a closure object nearly identical to the display closures described later in this dissertation [Car83, Car84]. The main difference is that Cardelli did not need to support assignment of variables. The use of ref cells in ML can replace the automatic generation of boxes for assigned variables described in this dissertation. There is no benefit in stack-allocating variable bindings if the stack itself is implemented in a heap. ML does not support general continuations, so this was not a problem for Cardelli.

## 1.3 Multiprocessor Systems and Implementations

Various computer systems have been proposed that provide multiple processor support for the concurrent execution, or parallel processing of subparts of a computer program. Parallel processing is often simulated on a single processor by interleaving the execution of subparts; this is often referred to as multi-tasking. Parallel processing and multi-tasking systems are both considered to be distributed processing systems. These have been studied in various forms for almost two decades; the earliest works on the subject are those of Dijkstra, Brinch Hansen, and Hoare [Dij68, Bri73, Bri78, Hoa76]. A review of distributed processing languages and abstractions can be found in Filman and Friedman’s text [Fil84].
General purpose(*2) multiple-processor systems (termed multiprocessors) can be partitioned into two categories based on the size of the processor and the complexity of the communication network: small-grain and large-grain. Large-grain multiprocessors usually contain from several to several hundred processing elements (PEs), whereas small-grain processors contain anywhere from hundreds to millions of PE’s. Each PE in a large-grain multiprocessor is typically the size of a minicomputer or powerful microprocessor. Each PE in a small-grain processor is typically the size of a small microprocessor or smaller, containing perhaps an ALU (arithmetic-logical unit), a few words of memory, and communication circuitry. Large-grain systems often employ a large central memory along with smaller local memories at each PE; small-grain systems usually do not have any central memory, relying only upon the local storage at the PEs and the temporary storage within the communication network.
One of the most important distinctions between large-grain and small-grain
systems is the type of communication network. Large-grain systems tend to be well-connected (they can afford to be); that is, access to shared memory and to each other is typically through a complex communication network that allows relatively high-speed packet-switched communication between any two processors.
In contrast, small-grain systems provide high-speed, circuit-switched communication between adjacent or nearly adjacent processors, but slow communication in general between two distant processors (because the processors are not wellconnected). Also, small amounts of information can be communicated efficiently; this is not typically the case in large-grain systems.
Large grain systems are most useful for programs where there is high level parallelism inherent in the structure of the program, that is, when a programmer or compiler can isolate several large sequential processes. It has yet to be seen whether there are effective methods for splitting a computation dynamically. Even with static analysis performed by a compiler, the results have not been impressive. As a result, most large-grain systems will be programmed by hand, a complex task that requires precious programmer time.
Small-grain systems should be more appropriate where the parallelism exists at a lower level, e.g., at the expression level rather than at the procedure level. Decomposition, or process-splitting, may be performed dynamically, since the cost of communicating locally is minimal, and the cost of shifting within the network can be minimized by parallel movement. Although small-grain systems look more promising than large-grain systems for some purposes, they have not yet been around long enough for their value to be proven. Mago ́ [Mag85] and Burton and Sleep [Bur81] provide convincing arguments for the viability of small-grain systems.
Several large-grain multiprocessors have been proposed for Lisp dialects. Marti and Fitch [Mar83] perform static analysis with a compiler to decompose Lisp programs; this seems to be one of the few attempts at executing Lisp programs on a multiprocessor without explicit programmer control over parallelism. Two recent contributions, one by Halstead [Hal84] (see also [And82]) and the other by Gabriel and McCarthy [Gab84], propose languages with explicit parallel control structures of similar natures. Sugimoto, et al. [Sug83] propose to perform some automatic program decomposition dynamically, while allowing access to lower level primitives.
Many small-grain multiprocessors have been proposed to support functional languages. These include string reduction machines such as the FFP machine, graph reduction machines such as ALICE [Dar81] and AMPS [Kel79], dataflow machines [Arv77, Den79], and the shared-memory “ultracomputer” [Got83]. Although all of these multiprocessors might be considered to be small-grain systems, relative to the FFP machine the others are actually large-grain systems.









----

1 A first-class object can be passed as an argument to a function, returned as the value of a function, and stored indefinitely. Most Lisp systems provide many types of first-class objects, including lists, symbols, strings, and numbers. Most other programming languages only provide scalar quantities, e.g., numbers and characters, as first-class objects.
2 We will not discuss special-purpose multiprocessors, such as systolic arrays or SIMD (single-instruction, multiple-data) machines, since we plan to support general-purpose programming where parallelism is not obvious in the data structures or operations of the language.


